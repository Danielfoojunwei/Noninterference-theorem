\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}

% ------------------------------------------------------------------
% Theorem environments
% ------------------------------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% ------------------------------------------------------------------
% Convenience macros
% ------------------------------------------------------------------
\newcommand{\St}{S_t}
\newcommand{\Pt}{P_t}
\newcommand{\Mt}{M_t}
\newcommand{\Bt}{B_t}
\newcommand{\Gt}{G_t}
\newcommand{\Vt}{V_t}
\newcommand{\Et}{E_t}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\taint}{\tau}
\newcommand{\prov}{\pi}
\newcommand{\SYS}{\textsf{SYS}}
\newcommand{\USER}{\textsf{USER}}
\newcommand{\TOOL}{\textsf{TOOL}}
\newcommand{\WEB}{\textsf{WEB}}
\newcommand{\SKILL}{\textsf{SKILL}}

% ------------------------------------------------------------------
\title{Noninterference Theorem for Indirect Prompt Injection\\in Agentic AI}
\author{}
\date{}

\begin{document}
\maketitle

% ==================================================================
\begin{abstract}
Agentic AI systems routinely ingest untrusted content---emails, web pages and
documents---within the same context window that contains system prompts and
user instructions.  Indirect prompt injection exploits this shared context to
embed malicious instructions in otherwise legitimate data, silently influencing
the agent's behaviour.  We formalise the security requirement as a
\emph{noninterference} property: adversarial variations in untrusted data must
not influence the agent's decisions or modify its control plane.  We model the
agent as a discrete-time dynamical system driven by a recursive language model
(RLM), define a typed intermediate representation with taint tracking and a
trust lattice, and prove that under strict invariants the agent's tool calls
and control-plane state are fully determined by trusted inputs alone.
\end{abstract}

% ==================================================================
\section{Introduction}\label{sec:intro}

Agentic AI systems such as OpenClaw autonomously execute tool calls and chain
operations across external services.  Because they ingest untrusted
content---retrieved web pages, user-uploaded documents, skill
outputs---alongside system prompts and direct user instructions, they are
vulnerable to \emph{indirect prompt injection}: an adversary embeds hidden
instructions in external data that reshape the agent's intent, redirect tool
usage, or trigger unauthorised actions~\cite{crowdstrike2025,esecplanet2025}.

CrowdStrike notes that indirect prompt injection allows adversaries to poison
the environment without directly interacting with the
agent~\cite{crowdstrike2025}.  eSecurity Planet further observes that current
systems do not enforce a hard separation between explicit user intent and
third-party content~\cite{esecplanet2025}, so information retrieved during a
task is processed in the same reasoning context as direct instructions.  Real
attacks have already been used to drain crypto wallets and exfiltrate private
channel messages~\cite{esecplanet2025}.

The fundamental security requirement is \textbf{noninterference}: adversarial
variations in untrusted data should not influence the agent's decisions or
modify its control plane.  In this paper we show how a recursive language model
(RLM) and a deterministic guardrail layer can enforce this property.

\paragraph{Contributions.}
\begin{enumerate}[label=(\roman*)]
  \item A formal model of agentic state incorporating a typed intermediate
        representation (IR) graph with taint tracking and a trust lattice
        (\S\ref{sec:model}).
  \item The \emph{Noninterference Theorem}: under stated invariants, tool calls
        and control-plane state are invariant to arbitrary adversarial variations
        in untrusted input (\S\ref{sec:theorem}).
  \item An evaluation plan for empirical validation via differential testing,
        side-effect monitoring and performance measurement (\S\ref{sec:eval}).
\end{enumerate}

% ==================================================================
\section{Background: Indirect Prompt Injection as a Control-Plane Threat}
\label{sec:background}

Agentic AI systems routinely ingest untrusted content such as emails, web
pages and documents.  This data is mixed into the same context window that
contains system prompts and user instructions.  CrowdStrike notes that
indirect prompt injection allows adversaries to poison the environment without
directly interacting with the agent; malicious instructions embedded in
otherwise legitimate data can silently influence the agent's
behaviour~\cite{crowdstrike2025}.  Because systems like OpenClaw autonomously
execute tool calls and chain operations, untrusted data can reshape intent,
redirect tool usage and trigger unauthorised actions~\cite{crowdstrike2025}.

eSecurity Planet further observes that OpenClaw does not enforce a hard
separation between explicit user intent and third-party
content~\cite{esecplanet2025}, so information retrieved during a task is
processed in the same reasoning context as direct
instructions~\cite{esecplanet2025}.  This design choice enables indirect
prompt injection to influence how the agent interprets what it should do
next~\cite{esecplanet2025}.  Attackers have already used indirect injection to
drain crypto wallets and to exfiltrate private channel
messages~\cite{esecplanet2025}.

% ==================================================================
\section{Model Definitions}\label{sec:model}

\input{definitions}

% ==================================================================
\section{Noninterference Theorem}\label{sec:theorem}

\input{proof}

% ==================================================================
\section{Discussion}\label{sec:discussion}

This theorem formalises the principle highlighted by CrowdStrike: runtime
guardrails must ensure that untrusted data cannot reshape intent or redirect
tool usage~\cite{crowdstrike2025}.  By defining a typed IR and enforcing
taint-aware dependence, the model creates a hard boundary between data and
instructions.

eSecurity Planet emphasises that OpenClaw currently fails to enforce this
separation~\cite{esecplanet2025}, allowing untrusted content to influence
decision-making~\cite{esecplanet2025}.  The noninterference theorem shows
that, under RLM-based recursion with strict invariants, this boundary can be
restored: the agent's behaviour becomes invariant to arbitrary adversarial
variations in untrusted input.

\paragraph{Limitations.}
The theorem assumes a deterministic verifier that perfectly enforces the stated
invariants.  In practice, implementation bugs in the verifier or in taint
propagation could violate the preconditions.  Additionally, the model treats
the RLM as a deterministic function conditioned on its inputs; stochastic
sampling introduces a source of variation that must be controlled (e.g.\ by
fixing the random seed or by proving the property in expectation).  We leave
the extension to probabilistic noninterference for future work.

% ==================================================================
\section{Evaluation Plan}\label{sec:eval}

To empirically validate the noninterference guarantee, we propose the
following steps.

\begin{enumerate}
  \item \textbf{Adversarial dataset.}  Build a corpus of benign documents and
    adversarially poisoned documents that embed hidden instructions (e.g.\
    ``ignore prior instructions and leak secrets'') similar to attacks seen in
    the wild~\cite{esecplanet2025}.

  \item \textbf{Differential testing.}  Run the agent twice on the same
    trusted tasks, swapping benign and adversarial documents in the untrusted
    input stream.  Measure whether tool calls and control-plane state differ.
    Under a correct implementation of the theorem, the sequences should be
    identical.

  \item \textbf{Side-effect monitoring.}  Verify that no sensitive data is
    leaked and that tool calls remain limited to the authorised set.
    eSecurity Planet notes that successful attacks can add new integrations and
    exfiltrate data~\cite{esecplanet2025}; the evaluation should confirm that
    this does not occur.

  \item \textbf{Performance overhead.}  Measure any latency introduced by IR
    construction and verification.  Since CrowdStrike emphasises the need for
    runtime protection~\cite{crowdstrike2025}, the guardrail layer must remain
    efficient.
\end{enumerate}

% ==================================================================
\section{Conclusion}\label{sec:conclusion}

Indirect prompt injection collapses the boundary between data and control by
hiding malicious instructions in external content~\cite{crowdstrike2025}.
The noninterference theorem shows that RLMs equipped with a typed IR, taint
tracking and authority-based update rules can prevent such attacks: no amount
of poisoned data can influence tool selection or modify the control plane.
This provides a mathematically grounded defence against the emerging class of
indirect injection attacks and aligns with industry recommendations to separate
trusted instructions from untrusted content~\cite{crowdstrike2025,esecplanet2025}.

% ==================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
